{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import librosa\n",
    "\n",
    "DATA_PATH = 'E:\\\\Datasets\\\\Speech\\\\train\\\\audio\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def wav2mfcc(file_path, max_len=11):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    wave = wave[::3]\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=16000)\n",
    "\n",
    "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
    "    if (max_len > mfcc.shape[1]):\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Else cutoff the remaining parts\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    \n",
    "    return mfcc\n",
    "\n",
    "def save_data_to_array(path=DATA_PATH, max_len=11):\n",
    "    labels, _, _ = get_labels(path)\n",
    "\n",
    "    for label in labels:\n",
    "        # Init mfcc vectors\n",
    "        mfcc_vectors = []\n",
    "\n",
    "        wavfiles = [path + label + '\\\\' + wavfile for wavfile in os.listdir(path + '\\\\' + label)]\n",
    "        for wavfile in tqdm(wavfiles, \"Saving vectors of label - '{}'\".format(label)):\n",
    "            mfcc = wav2mfcc(wavfile, max_len=max_len)\n",
    "            mfcc_vectors.append(mfcc)\n",
    "        np.save('data\\\\speech\\\\'+label + '.npy', mfcc_vectors)\n",
    "        print('saved data : ', label)\n",
    "        \n",
    "save_data_to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def get_labels(path=DATA_PATH):\n",
    "    labels = os.listdir(path)\n",
    "    label_indices = np.arange(0, len(labels))\n",
    "    return labels, label_indices, to_categorical(label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['bed',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'dog',\n",
       "  'down',\n",
       "  'eight',\n",
       "  'five',\n",
       "  'four',\n",
       "  'go',\n",
       "  'happy',\n",
       "  'house',\n",
       "  'left',\n",
       "  'marvin',\n",
       "  'nine',\n",
       "  'no',\n",
       "  'off',\n",
       "  'on',\n",
       "  'one',\n",
       "  'right',\n",
       "  'seven',\n",
       "  'sheila',\n",
       "  'six',\n",
       "  'stop',\n",
       "  'three',\n",
       "  'tree',\n",
       "  'two',\n",
       "  'up',\n",
       "  'wow',\n",
       "  'yes',\n",
       "  'zero'],\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       " array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  1.]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_train_test(split_ratio=0.9, random_state=42):\n",
    "    # get available class labels\n",
    "    labels, indices, _ = get_labels()\n",
    "    \n",
    "    # Getting first arrays\n",
    "    X = np.load('data\\\\speech\\\\'+labels[0] + '.npy')\n",
    "    y = np.zeros(X.shape[0])\n",
    "\n",
    "    # Append all of the dataset into one single array, same goes for y\n",
    "    for i, label in enumerate(labels[1:]):\n",
    "        x = np.load('data\\\\speech\\\\'+label + '.npy')\n",
    "        X = np.vstack((X, x))\n",
    "        y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
    "\n",
    "    assert X.shape[0] == len(y)\n",
    "\n",
    "    return train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58248, 20, 11)\n",
      "(6473, 20, 11)\n",
      "(58248,)\n",
      "(6473,)\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = get_train_test()\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58248, 20, 11, 1)\n",
      "(6473, 20, 11, 1)\n",
      "(58248, 30)\n",
      "(6473, 30)\n"
     ]
    }
   ],
   "source": [
    "x, y, z = train_x.shape\n",
    "train_x = train_x.reshape(x, y, z, 1)\n",
    "test_x = test_x.reshape(test_x.shape[0], y, z, 1)\n",
    "\n",
    "train_y_categorical = to_categorical(train_y)\n",
    "test_y_categorical = to_categorical(test_y)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_y_categorical.shape)\n",
    "print(test_y_categorical.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the netowrk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 19, 10, 32)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1440)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               184448    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                3870      \n",
      "=================================================================\n",
      "Total params: 188,478\n",
      "Trainable params: 188,478\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(20, 11, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(30, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.fit(train_x, train_y_categorical, batch_size=100, epochs=200, validation_data=(test_x, test_y_categorical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58248, 20, 11)\n",
      "(6473, 20, 11)\n",
      "(58248,)\n",
      "(6473,)\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = get_train_test()\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_2 (GRU)                  (None, 128)               53760     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                3870      \n",
      "=================================================================\n",
      "Total params: 57,630\n",
      "Trainable params: 57,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.GRU(128, activation='relu', input_shape=(None, 11)))\n",
    "model.add(layers.Dense(30, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58248 samples, validate on 6473 samples\n",
      "Epoch 1/50\n",
      "58248/58248 [==============================] - 15s 257us/step - loss: 2.0098 - categorical_accuracy: 0.3818 - val_loss: 1.9198 - val_categorical_accuracy: 0.4080\n",
      "Epoch 2/50\n",
      "58248/58248 [==============================] - 11s 182us/step - loss: 1.7420 - categorical_accuracy: 0.4641 - val_loss: 1.7367 - val_categorical_accuracy: 0.4661\n",
      "Epoch 3/50\n",
      "58248/58248 [==============================] - 12s 204us/step - loss: 1.5528 - categorical_accuracy: 0.5181 - val_loss: 1.5868 - val_categorical_accuracy: 0.5081\n",
      "Epoch 4/50\n",
      "58248/58248 [==============================] - 14s 233us/step - loss: 1.4219 - categorical_accuracy: 0.5614 - val_loss: 1.4807 - val_categorical_accuracy: 0.5494\n",
      "Epoch 5/50\n",
      "58248/58248 [==============================] - 14s 233us/step - loss: 1.3219 - categorical_accuracy: 0.5937 - val_loss: 1.4189 - val_categorical_accuracy: 0.5758\n",
      "Epoch 6/50\n",
      "58248/58248 [==============================] - 14s 232us/step - loss: 1.2431 - categorical_accuracy: 0.6183 - val_loss: 1.3233 - val_categorical_accuracy: 0.6017\n",
      "Epoch 7/50\n",
      "58248/58248 [==============================] - 14s 235us/step - loss: 1.1678 - categorical_accuracy: 0.6386 - val_loss: 1.2977 - val_categorical_accuracy: 0.6119\n",
      "Epoch 8/50\n",
      "58248/58248 [==============================] - 13s 227us/step - loss: 1.1059 - categorical_accuracy: 0.6571 - val_loss: 1.2537 - val_categorical_accuracy: 0.6272\n",
      "Epoch 9/50\n",
      "58248/58248 [==============================] - 13s 228us/step - loss: 1.0577 - categorical_accuracy: 0.6737 - val_loss: 1.1902 - val_categorical_accuracy: 0.6417\n",
      "Epoch 10/50\n",
      "58248/58248 [==============================] - 14s 242us/step - loss: 1.0076 - categorical_accuracy: 0.6890 - val_loss: 1.1767 - val_categorical_accuracy: 0.6499\n",
      "Epoch 11/50\n",
      "58248/58248 [==============================] - 14s 248us/step - loss: 0.9680 - categorical_accuracy: 0.7004 - val_loss: 1.1708 - val_categorical_accuracy: 0.6450\n",
      "Epoch 12/50\n",
      "58248/58248 [==============================] - 15s 251us/step - loss: 0.9342 - categorical_accuracy: 0.7096 - val_loss: 1.1475 - val_categorical_accuracy: 0.6543\n",
      "Epoch 13/50\n",
      "58248/58248 [==============================] - 12s 204us/step - loss: 0.8977 - categorical_accuracy: 0.7218 - val_loss: 1.1064 - val_categorical_accuracy: 0.6712\n",
      "Epoch 14/50\n",
      "58248/58248 [==============================] - 11s 186us/step - loss: 0.8646 - categorical_accuracy: 0.7296 - val_loss: 1.1089 - val_categorical_accuracy: 0.6654\n",
      "Epoch 15/50\n",
      "58248/58248 [==============================] - 11s 185us/step - loss: 0.8445 - categorical_accuracy: 0.7356 - val_loss: 1.1043 - val_categorical_accuracy: 0.6663\n",
      "Epoch 16/50\n",
      "58248/58248 [==============================] - 11s 184us/step - loss: 0.8151 - categorical_accuracy: 0.7449 - val_loss: 1.0863 - val_categorical_accuracy: 0.6830\n",
      "Epoch 17/50\n",
      "58248/58248 [==============================] - 11s 186us/step - loss: 0.7974 - categorical_accuracy: 0.7503 - val_loss: 1.0762 - val_categorical_accuracy: 0.6759\n",
      "Epoch 18/50\n",
      "58248/58248 [==============================] - 11s 186us/step - loss: 0.7762 - categorical_accuracy: 0.7569 - val_loss: 1.0727 - val_categorical_accuracy: 0.6839\n",
      "Epoch 19/50\n",
      "58248/58248 [==============================] - 11s 186us/step - loss: 0.7631 - categorical_accuracy: 0.7617 - val_loss: 1.0536 - val_categorical_accuracy: 0.6896\n",
      "Epoch 20/50\n",
      "58248/58248 [==============================] - 13s 220us/step - loss: 0.7410 - categorical_accuracy: 0.7701 - val_loss: 1.0584 - val_categorical_accuracy: 0.6923\n",
      "Epoch 21/50\n",
      "58248/58248 [==============================] - 15s 252us/step - loss: 0.7240 - categorical_accuracy: 0.7735 - val_loss: 1.0730 - val_categorical_accuracy: 0.6893\n",
      "Epoch 22/50\n",
      "58248/58248 [==============================] - 16s 267us/step - loss: 0.7130 - categorical_accuracy: 0.7775 - val_loss: 1.0561 - val_categorical_accuracy: 0.6930\n",
      "Epoch 23/50\n",
      "58248/58248 [==============================] - 14s 241us/step - loss: 0.6980 - categorical_accuracy: 0.7807 - val_loss: 1.0250 - val_categorical_accuracy: 0.6923\n",
      "Epoch 24/50\n",
      "58248/58248 [==============================] - 13s 223us/step - loss: 0.6875 - categorical_accuracy: 0.7840 - val_loss: 1.0533 - val_categorical_accuracy: 0.6937\n",
      "Epoch 25/50\n",
      "58248/58248 [==============================] - 13s 221us/step - loss: 0.6655 - categorical_accuracy: 0.7916 - val_loss: 1.0242 - val_categorical_accuracy: 0.7054\n",
      "Epoch 26/50\n",
      "58248/58248 [==============================] - 15s 252us/step - loss: 0.6609 - categorical_accuracy: 0.7903 - val_loss: 1.0353 - val_categorical_accuracy: 0.6935\n",
      "Epoch 27/50\n",
      "58248/58248 [==============================] - 15s 249us/step - loss: 0.6495 - categorical_accuracy: 0.7951 - val_loss: 1.0634 - val_categorical_accuracy: 0.6926\n",
      "Epoch 28/50\n",
      "58248/58248 [==============================] - 13s 229us/step - loss: 0.6459 - categorical_accuracy: 0.7957 - val_loss: 1.0341 - val_categorical_accuracy: 0.7032\n",
      "Epoch 29/50\n",
      "58248/58248 [==============================] - 15s 257us/step - loss: 0.6351 - categorical_accuracy: 0.8011 - val_loss: 1.1050 - val_categorical_accuracy: 0.6882\n",
      "Epoch 30/50\n",
      "58248/58248 [==============================] - 15s 258us/step - loss: 0.6314 - categorical_accuracy: 0.7998 - val_loss: 1.0587 - val_categorical_accuracy: 0.6958\n",
      "Epoch 31/50\n",
      "58248/58248 [==============================] - 13s 231us/step - loss: 0.6075 - categorical_accuracy: 0.8067 - val_loss: 1.0536 - val_categorical_accuracy: 0.6970\n",
      "Epoch 32/50\n",
      "58248/58248 [==============================] - 13s 229us/step - loss: 0.6042 - categorical_accuracy: 0.8088 - val_loss: 1.0547 - val_categorical_accuracy: 0.6998\n",
      "Epoch 33/50\n",
      "58248/58248 [==============================] - 14s 245us/step - loss: 0.6034 - categorical_accuracy: 0.8093 - val_loss: 1.0366 - val_categorical_accuracy: 0.7031\n",
      "Epoch 34/50\n",
      "58248/58248 [==============================] - 14s 249us/step - loss: 0.5905 - categorical_accuracy: 0.8134 - val_loss: 1.0559 - val_categorical_accuracy: 0.7102\n",
      "Epoch 35/50\n",
      "58248/58248 [==============================] - 13s 232us/step - loss: 0.5851 - categorical_accuracy: 0.8143 - val_loss: 1.0820 - val_categorical_accuracy: 0.7026\n",
      "Epoch 36/50\n",
      "58248/58248 [==============================] - 13s 225us/step - loss: 0.5749 - categorical_accuracy: 0.8164 - val_loss: 1.0846 - val_categorical_accuracy: 0.7032\n",
      "Epoch 37/50\n",
      "58248/58248 [==============================] - 13s 229us/step - loss: 0.5784 - categorical_accuracy: 0.8158 - val_loss: 1.0688 - val_categorical_accuracy: 0.7059\n",
      "Epoch 38/50\n",
      "58248/58248 [==============================] - 13s 222us/step - loss: 0.5679 - categorical_accuracy: 0.8185 - val_loss: 1.0723 - val_categorical_accuracy: 0.7060\n",
      "Epoch 39/50\n",
      "58248/58248 [==============================] - 13s 231us/step - loss: 0.5676 - categorical_accuracy: 0.8175 - val_loss: 1.0705 - val_categorical_accuracy: 0.7012\n",
      "Epoch 40/50\n",
      "58248/58248 [==============================] - 13s 226us/step - loss: 0.5613 - categorical_accuracy: 0.8214 - val_loss: 1.1168 - val_categorical_accuracy: 0.7021\n",
      "Epoch 41/50\n",
      "58248/58248 [==============================] - 13s 231us/step - loss: 0.5581 - categorical_accuracy: 0.8204 - val_loss: 1.0598 - val_categorical_accuracy: 0.7145\n",
      "Epoch 42/50\n",
      "58248/58248 [==============================] - 13s 229us/step - loss: 0.5416 - categorical_accuracy: 0.8257 - val_loss: 1.0619 - val_categorical_accuracy: 0.7088\n",
      "Epoch 43/50\n",
      "58248/58248 [==============================] - 14s 239us/step - loss: 0.5447 - categorical_accuracy: 0.8244 - val_loss: 1.0866 - val_categorical_accuracy: 0.7080\n",
      "Epoch 44/50\n",
      "58248/58248 [==============================] - 15s 251us/step - loss: 0.5279 - categorical_accuracy: 0.8309 - val_loss: 1.0713 - val_categorical_accuracy: 0.7139\n",
      "Epoch 45/50\n",
      "58248/58248 [==============================] - 14s 240us/step - loss: 0.5414 - categorical_accuracy: 0.8272 - val_loss: 1.0995 - val_categorical_accuracy: 0.7052\n",
      "Epoch 46/50\n",
      "58248/58248 [==============================] - 13s 227us/step - loss: 0.5297 - categorical_accuracy: 0.8295 - val_loss: 1.0999 - val_categorical_accuracy: 0.7045\n",
      "Epoch 47/50\n",
      "58248/58248 [==============================] - 13s 232us/step - loss: 0.5235 - categorical_accuracy: 0.8326 - val_loss: 1.1058 - val_categorical_accuracy: 0.7113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "58248/58248 [==============================] - 13s 227us/step - loss: 0.5222 - categorical_accuracy: 0.8316 - val_loss: 1.0971 - val_categorical_accuracy: 0.7106\n",
      "Epoch 49/50\n",
      "58248/58248 [==============================] - 14s 234us/step - loss: 0.5192 - categorical_accuracy: 0.8340 - val_loss: 1.0953 - val_categorical_accuracy: 0.7060\n",
      "Epoch 50/50\n",
      "58248/58248 [==============================] - 14s 237us/step - loss: 0.5202 - categorical_accuracy: 0.8315 - val_loss: 1.0793 - val_categorical_accuracy: 0.7142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22181366908>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=5e-4),\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.fit(train_x, train_y_categorical, batch_size=100, epochs=50, validation_data=(test_x, test_y_categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
